{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76852df3-783e-4504-9863-51c46adbe624",
   "metadata": {},
   "source": [
    "# Classification d'image vue de face de veaux avec Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a17a0e-13f5-4185-8ef8-dfdacaef5a16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Import des dependances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c2159c-e743-4622-a4b8-3ce6187d34ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-27 16:37:00.573954: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-27 16:37:00.584840: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-27 16:37:00.588159: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-27 16:37:00.597361: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-27 16:37:01.302925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.segmentation as segmentation\n",
    "from torchvision.transforms import v2\n",
    "from torchvision.io import read_image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from PIL import Image as PilImage\n",
    "from omnixai.data.image import Image\n",
    "from omnixai.explainers.vision import LimeImage, VisionExplainer\n",
    "from omnixai.preprocessing.image import Resize\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from IPython.display import clear_output\n",
    "import timm\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Local dep\n",
    "project_dir = '/data/konrad/workspace'\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "from helpers.helpers import get_indices, load_face_data, load_local_model\n",
    "from helpers.datasets import CalfCenterFaceDataset\n",
    "from helpers.interp import GradCam, generate_cam, display_predicted_cam\n",
    "from helpers.trainers import train_model, validate_model, plot_metrics\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d89cf51-b6ad-40c9-a94a-3110e666b664",
   "metadata": {},
   "source": [
    "## Classes et utilitaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc6d97a-9dde-49b9-aa4c-ca1a622b7d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Video Dataset Class\n",
    "class CalfVideoDataset(Dataset):\n",
    "    def __init__(self, video_df, images_df, use_face=False, max_frames=5, bbox_size = 800, transform = None):\n",
    "        self.video_df = video_df.reset_index(drop=True)\n",
    "        self.images_df = images_df\n",
    "        self.images_df[\"id\"] = images_df.index\n",
    "        self.max_frames = max_frames\n",
    "        self.bbox_size = bbox_size\n",
    "        self.transform = transform\n",
    "        self.use_face = use_face\n",
    "\n",
    "        self.labels = self.video_df[\"target\"]\n",
    "        \n",
    "        label_to_count = self.labels.value_counts()\n",
    "\n",
    "        weights = 1.0 / label_to_count[self.labels]\n",
    "\n",
    "        self.weights = weights.to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_df)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def get_class_weights(self):\n",
    "        return torch.FloatTensor(1.0 / self.labels.value_counts())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_video = self.video_df.loc[idx]\n",
    "        label = torch.tensor(row_video[\"target\"], dtype=torch.float32)\n",
    "        images = self.images_df[self.images_df[\"video\"] == row_video[\"video\"]]\n",
    "        images = images.sample(self.max_frames).sort_values(by=\"id\")\n",
    "        \n",
    "        frames = []\n",
    "        for i, row in images.iterrows():\n",
    "            image = PilImage.open(row[\"path\"])\n",
    "            \n",
    "            xmin = row['x_min']\n",
    "            ymin = row['y_min']\n",
    "            xmax = row['x_max']\n",
    "            ymax = row['y_max']\n",
    "            \n",
    "            bbox_cropped_image = image.crop((xmin, ymin, xmax, ymax))\n",
    "\n",
    "            image = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])(image)\n",
    "            bbox_cropped_image = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])(bbox_cropped_image)\n",
    "            \n",
    "            # Apply transformations if provided\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                bbox_cropped_image = self.transform(bbox_cropped_image)\n",
    "    \n",
    "            frames.append(bbox_cropped_image if self.use_face else image)\n",
    "\n",
    "        # Plot each image\n",
    "        # for i, img in enumerate(frames):\n",
    "        #     plt.figure()\n",
    "        #     plt.imshow(img.permute(1, 2, 0))  # Normalize for display\n",
    "        #     plt.title(f\"Image {i+1}\")\n",
    "        #     plt.axis('off')  # Hide axes\n",
    "        #     plt.show()\n",
    "        video_tensor = torch.stack(frames, dim=0)\n",
    "\n",
    "        return {\"image\": video_tensor.to(device), \"label\": label.to(device), \"weight\": self.weights[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48a525ca-4d18-46ea-8dab-68f53283fb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, \n",
    "             num_classes, name, \n",
    "             from_pretrained=True, \n",
    "             method='sqeeze', \n",
    "             dr_rate= 0.2, \n",
    "             rnn_hidden_size = 30,\n",
    "             rnn_num_layers = 2,\n",
    "             fc_size = 256,\n",
    "             timestep = 4\n",
    "        ):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.method = method\n",
    "        self.name = name\n",
    "        \n",
    "        baseModel = models.vgg19(pretrained=from_pretrained).features\n",
    "        i = 0\n",
    "        for child in baseModel.children():\n",
    "            for param in child.parameters():\n",
    "            # if i < 28:\n",
    "            #     for param in child.parameters():\n",
    "                    \n",
    "            # else:\n",
    "            #     for param in child.parameters():\n",
    "            #         param.requires_grad = True\n",
    "            # i +=1\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.baseModel = baseModel\n",
    "        self.dropout= nn.Dropout(dr_rate)\n",
    "        # 25088 = 512x7x7\n",
    "        self.rnn = nn.LSTM(25088, rnn_hidden_size, rnn_num_layers , batch_first=True)\n",
    "        self.fc1 = nn.Linear(rnn_hidden_size, fc_size)\n",
    "        self.fc2 = nn.Linear(timestep, num_classes)\n",
    " \n",
    "    def forward(self, x):\n",
    "        batch_size, time_steps, C, H, W = x.size()\n",
    "        # reshape input  to be (batch_size * timesteps, input_size)\n",
    "        x = x.contiguous().view(batch_size * time_steps, C, H, W)\n",
    "        x = self.baseModel(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #make output as  ( samples, timesteps, output_size)\n",
    "        x = x.contiguous().view(batch_size , time_steps , x.size(-1))\n",
    "        x , (hn, cn) = self.rnn(x)\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.fc1(x[:, -1, :])) # get output of the last  lstm not full sequence\n",
    "        # x = self.dropout(x)\n",
    "        print(x.shape)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.fc2(x)\n",
    "        print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8f0d1-574f-419b-bd50-65c65517aa13",
   "metadata": {},
   "source": [
    "## Entrainement du modele"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9062e11-0054-4664-bd96-03b762db03f8",
   "metadata": {},
   "source": [
    "### Dataset setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9d6b87-5850-42a8-b2d4-069cc7d8173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/data/konrad/workspace\"\n",
    "IMAGE_SIZE = 224\n",
    "train_on_face = False\n",
    "image_per_video = 4\n",
    "\n",
    "dataset_type = \"subsplt\"\n",
    "image_type = \"splt\"\n",
    "video_type = \"subsplt\"\n",
    "\n",
    "train_images_df = pd.read_csv(ROOT_DIR + f'/csv_files/mixed_10s_b0s_y7_1/{image_type}_train_image_extracted_metadata.csv', index_col=False)\n",
    "valid_images_df = pd.read_csv(ROOT_DIR + f'/csv_files/mixed_10s_b0s_y7_1/{image_type}_test_image_extracted_metadata.csv', index_col=False)\n",
    "test_images_df = pd.read_csv(ROOT_DIR + f'/csv_files/mixed_10s_b0s_y7_1/{image_type}_val_image_extracted_metadata.csv', index_col=False)\n",
    "training_images_df = pd.concat([train_images_df, valid_images_df], ignore_index=True)\n",
    "\n",
    "train_videos_df = pd.read_csv(ROOT_DIR + f'/csv_files/mixed_10s_b0s_y7_1/{video_type}_train_video_extracted_metadata.csv', index_col=False)\n",
    "valid_videos_df = pd.read_csv(ROOT_DIR + f'/csv_files/mixed_10s_b0s_y7_1/{video_type}_test_video_extracted_metadata.csv', index_col=False)\n",
    "test_videos_df = pd.read_csv(ROOT_DIR + f'/csv_files/mixed_10s_b0s_y7_1/{video_type}_val_video_extracted_metadata.csv', index_col=False)\n",
    "\n",
    "# label_col = \"bilabel\"\n",
    "\n",
    "label_col = \"label\"\n",
    "train_videos_df = train_videos_df[~ (train_videos_df[label_col] == \"Diarrhé, Pneumonie\")]\n",
    "valid_videos_df = valid_videos_df[~ (valid_videos_df[label_col] == \"Diarrhé, Pneumonie\")]\n",
    "test_videos_df = test_videos_df[~ (test_videos_df[label_col] == \"Diarrhé, Pneumonie\")]\n",
    "\n",
    "labels = train_videos_df[label_col].unique()\n",
    "label2id = {l:i for i, l in enumerate(labels)}\n",
    "id2label = {i:l for i, l in enumerate(labels)}\n",
    "\n",
    "num_labels = len(labels)\n",
    "# num_labels = 1\n",
    "\n",
    "train_videos_df['target'] = train_videos_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "valid_videos_df['target'] = valid_videos_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "test_videos_df['target'] = test_videos_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "\n",
    "\n",
    "# train_transform = v2.Compose([\n",
    "#     v2.Resize(size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
    "#     v2.RandomHorizontalFlip(p=0.5),\n",
    "#     v2.AutoAugment(v2.AutoAugmentPolicy.IMAGENET),\n",
    "#     v2.ToDtype(torch.float32, scale=True),\n",
    "#     v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "# ])\n",
    "test_transform = v2.Compose([\n",
    "    v2.Resize(size=(IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = CalfVideoDataset(train_videos_df, training_images_df, use_face=train_on_face, max_frames=image_per_video,  bbox_size = IMAGE_SIZE, transform=test_transform)\n",
    "test_dataset = CalfVideoDataset(test_videos_df, training_images_df, use_face=train_on_face, max_frames=image_per_video,  bbox_size = IMAGE_SIZE, transform=test_transform)\n",
    "valid_dataset = CalfVideoDataset(valid_videos_df, test_images_df, use_face=train_on_face, max_frames=image_per_video,  bbox_size = IMAGE_SIZE, transform=test_transform)\n",
    "\n",
    "# Data loaders\n",
    "batch_size = 12\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=ImbalancedDatasetSampler(train_dataset))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size * 2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size * 2, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dce62f-83cc-4bc7-a396-8ef996eb1d41",
   "metadata": {},
   "source": [
    "### Entrainement et validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0d43b6e-c6ba-414e-bc32-72f5f273be88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/konrad/x_env/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/konrad/x_env/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 372 and validating on 94 datas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/31 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 588.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     average \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m ROOT_DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/training_log\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 26\u001b[0m metrics_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m clear_output()\n\u001b[1;32m     28\u001b[0m plot_metrics(metrics_scores)\n",
      "File \u001b[0;32m~/workspace/helpers/trainers.py:32\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, labels_name, train_loader, valid_loader, scheduler, patience, optimizer, output_dir, num_epochs, average)\u001b[0m\n\u001b[1;32m     29\u001b[0m weights \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(labels_name) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     35\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCEWithLogitsLoss(weight\u001b[38;5;241m=\u001b[39mweights\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/x_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/x_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# reshape input  to be (batch_size * timesteps, input_size)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m time_steps, C, H, W)\n\u001b[0;32m---> 40\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbaseModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#make output as  ( samples, timesteps, output_size)\u001b[39;00m\n",
      "File \u001b[0;32m~/x_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/x_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/x_env/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/x_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/x_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/x_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/x_env/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 588.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "model_name = f\"lstm_cnn_{dataset_type}_{label_col}_{batch_size}\"\n",
    "model = Classifier(\n",
    "    num_classes=num_labels, name=model_name, from_pretrained=True,\n",
    "    method='sqeeze', \n",
    "    dr_rate= 0.3, \n",
    "    rnn_hidden_size = 40,\n",
    "    rnn_num_layers = 1,\n",
    "    fc_size = 160,\n",
    ")  # Adjust num_classes according to your dataset\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "lr = 5e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "epochs = 50\n",
    "patience = 5\n",
    "\n",
    "if num_labels == 1:\n",
    "    average = \"binary\"\n",
    "else:\n",
    "    average = \"weighted\"\n",
    "\n",
    "output_dir = ROOT_DIR + \"/training_log\"\n",
    "metrics_scores = train_model(model, labels, train_loader, test_loader, scheduler, patience, optimizer, output_dir, num_epochs = epochs, average=average)\n",
    "clear_output()\n",
    "plot_metrics(metrics_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ccb7e-015c-41e6-879b-e03e9559779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(f'{output_dir}/models/best_{model.name}_model.pth'))\n",
    "validate_model(model, valid_loader, labels, average=average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e77857-b54b-4fa6-b412-d0e1475c1480",
   "metadata": {},
   "source": [
    "## Interpretation with Lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3aa30-0475-4440-a377-5cb211297752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Classifier(num_classes=len(labels), name=model_name, from_pretrained=False)  # Adjust num_classes according to your dataset\n",
    "# model.load_state_dict(torch.load(f'{output_dir}/models/best_{model.name}_model.pth'))\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553f7d47-6e13-4cdc-ac7d-a034edc5c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = valid_loader.dataset\n",
    "# max_item = 20\n",
    "# test_images = [Resize((256, 256)).transform(Image(test_df[p][\"image\"].cpu().numpy(), channel_last = False)).to_numpy() for p in range(max_item)]\n",
    "# img = Image( data=np.concatenate(test_images), batched=True)\n",
    "\n",
    "# # The preprocessing function\n",
    "# transform = v2.Compose([\n",
    "#     v2.Resize(256),\n",
    "#     v2.CenterCrop(224),\n",
    "#     v2.ToImage(),\n",
    "#     v2.ToDtype(torch.float32, scale=True),\n",
    "#     v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "# preprocess = lambda ims: torch.stack([transform(im.to_pil()) for im in ims]).to(device)\n",
    "# postprocess = lambda logits: torch.nn.functional.softmax(logits, dim=1)\n",
    "\n",
    "# # target_head = model.conv_head\n",
    "# # target_layer = model.conv_head\n",
    "# explainer = VisionExplainer(\n",
    "#     explainers=[\n",
    "#         # \"gradcam\",\n",
    "#         \"lime\", \n",
    "#         # \"ig\",\n",
    "#         # \"ce\",\n",
    "#         # \"scorecam\",\n",
    "#         # \"smoothgrad\", \n",
    "#         # \"guidedbp\", \n",
    "#         # \"layercam\"\n",
    "#     ],\n",
    "#     mode=\"classification\",\n",
    "#     model=model,\n",
    "#     preprocess=preprocess,\n",
    "#     postprocess=postprocess,\n",
    "#     params={\n",
    "#         # \"gradcam\": {\"target_layer\": target_head},\n",
    "#         # \"ce\": {\"binary_search_steps\": 2, \"num_iterations\": 100},\n",
    "#         # \"scorecam\": {\"target_layer\": target_head},\n",
    "#         # \"layercam\": {\"target_layer\": target_layer},\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# # Generate explanations\n",
    "# local_explanations = explainer.explain(img)\n",
    "# clear_output()\n",
    "\n",
    "# for row_id in range(max_item):\n",
    "#     row = test_df[row_id]\n",
    "#     print(f\"Interpretation pour image {row_id}: \\n\")\n",
    "#     plt.imshow(row[\"image\"].cpu().permute(1, 2, 0))\n",
    "#     plt.show()\n",
    "#     print(f\"De classe: {id2label[row['label'].cpu().item()]}: \\n\")\n",
    "#     for name, explanations in local_explanations.items():\n",
    "#         print(f\"{name}:\")\n",
    "#         explanations.ipython_plot(row_id, class_names=labels)\n",
    "\n",
    "# print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

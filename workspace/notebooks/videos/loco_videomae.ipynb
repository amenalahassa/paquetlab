{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1519f9-3055-4767-8dee-211c9f7c1280",
   "metadata": {
    "editable": true,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Dependances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666f0e23-1f58-4da0-b754-d814ae4b3f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 13:33:35.196515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-31 13:33:35.207026: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-31 13:33:35.210243: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-31 13:33:35.218418: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-31 13:33:35.866510: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, balanced_accuracy_score\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local dep\n",
    "project_dir = '/data/konrad/workspace'\n",
    "sys.path.insert(0, project_dir)\n",
    "from helpers.helpers import get_indices, load_local_model\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6387ec-50e8-4503-b1d6-6a41a71fe9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Video Dataset Class\n",
    "class CalfVideoDataset(Dataset):\n",
    "    def __init__(self, video_df, processor, max_frames=20, frame_rate = 4, transform = None):\n",
    "        self.video_df = video_df\n",
    "        self.processor = processor\n",
    "        self.max_frames = max_frames\n",
    "        self.frame_rate = frame_rate\n",
    "        self.transform = transform\n",
    "\n",
    "        self.labels = self.video_df[\"target\"]\n",
    "        \n",
    "        label_to_count = self.labels.value_counts()\n",
    "\n",
    "        weights = 1.0 / label_to_count[self.labels]\n",
    "\n",
    "        self.weights = weights.to_list()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_df)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_video = self.video_df.loc[idx]\n",
    "        label = torch.tensor(row_video[\"target\"], dtype=torch.float32)\n",
    "        container = av.open(row_video[\"path\"])\n",
    "\n",
    "        video_stream = container.streams.video[0]\n",
    "\n",
    "        # Get the duration in time base units\n",
    "        duration_in_units = video_stream.duration\n",
    "        \n",
    "        # Get the time base\n",
    "        time_base = video_stream.time_base\n",
    "        \n",
    "        # Calculate the duration in seconds\n",
    "        duration_in_seconds = duration_in_units * time_base\n",
    "\n",
    "        # Sample 16 frames\n",
    "        fps = int(container.streams.video[0].average_rate)\n",
    "        seg_len = int(duration_in_seconds) * fps\n",
    "        sample_rate = min(int(seg_len / self.max_frames) - 1, self.frame_rate)\n",
    "\n",
    "        # sample max_frames frames\n",
    "        indices = sample_frame_indices(clip_len=self.max_frames, frame_sample_rate=sample_rate, seg_len=seg_len)\n",
    "        video = read_video_pyav(container=container, indices=indices)\n",
    "\n",
    "        # if self.transform:\n",
    "        #     video = self.transform(video.transpose(0, 3, 1, 2))\n",
    "        \n",
    "        inputs = self.processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "        return {\"pixel_values\": inputs[\"pixel_values\"][0], \"label\": label, \"weight\": self.weights[idx]}\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        weights = inputs.pop(\"weights\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # print(logits.shape)\n",
    "        # compute custom loss for 3 labels with different weights\n",
    "        if self.model.config.num_labels > 1:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=weights.to(model.device))\n",
    "        else:\n",
    "            loss_fct = nn.BCEWithLogitsLoss(weight=weights.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels).flatten(), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "    def get_train_dataloader(self):\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset \n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "            \"sampler\": ImbalancedDatasetSampler(train_dataset)\n",
    "        }\n",
    "\n",
    "        return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2424d25b-e0d2-4003-b5a3-86d08ddfc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a collate function to handle the feature extractor outputs\n",
    "def collate_fn(batch):\n",
    "    # print(torch.stack([item['pixel_values'] for item in batch]).shape)\n",
    "    # print(batch)\n",
    "    return {\n",
    "        'pixel_values': torch.stack([item['pixel_values'] for item in batch]),\n",
    "        'labels': torch.tensor([item['label'] for item in batch]),\n",
    "        'weights': torch.tensor([item['weight'] for item in batch]),\n",
    "    }\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    # preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    if num_labels == 1:\n",
    "        # For binary classification, assume preds are probabilities and use threshold 0.5\n",
    "        # print(pred.predictions.shape, pred.predictions)\n",
    "        probs = torch.sigmoid(torch.tensor(pred.predictions))\n",
    "        preds = (probs > 0.5).numpy().astype(int).squeeze()\n",
    "    else:\n",
    "        # For multi-class classification, take the argmax to get the predicted class labels\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    metric_type = \"binary\" if num_labels == 1 else \"macro\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=metric_type)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    bal_acc = balanced_accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        \"balanced_accuracy\": bal_acc\n",
    "    }\n",
    "\n",
    "def compute_class_metrics(pred):\n",
    "    out = {}\n",
    "    labels = pred.label_ids\n",
    "    \n",
    "    if num_labels == 1:\n",
    "        # For binary classification, assume preds are probabilities and use threshold 0.5\n",
    "        probs = torch.sigmoid(torch.tensor(pred.predictions))\n",
    "        preds = (probs > 0.5).numpy().astype(int).squeeze()\n",
    "    else:\n",
    "        # For multi-class classification, take the argmax to get the predicted class labels\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # preds = pred.predictions.argmax(-1)\n",
    "    metric_type = \"binary\" if num_labels == 1 else \"micro\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=metric_type)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    report = classification_report(labels, preds, target_names=id2label.values(), output_dict=True)\n",
    "    bal_acc = balanced_accuracy_score(labels, preds)\n",
    "\n",
    "    if num_labels > 1:\n",
    "        for class_name, metrics in report.items():\n",
    "            if class_name not in ('accuracy', 'macro avg', 'weighted avg'):\n",
    "                out[f\"{class_name}_precision\"] = metrics['precision']\n",
    "                out[f\"{class_name}_recall\"] = metrics['recall']\n",
    "                out[f\"{class_name}_f1-score\"] = metrics['f1-score']\n",
    "\n",
    "    out.update({\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        \"balanced_accuracy\": bal_acc\n",
    "    })\n",
    "    \n",
    "    return out\n",
    "    \n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    # start_idx = end_idx - converted_len\n",
    "    start_idx = 0\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04116f36-81c2-4403-800c-90a82c87c5fd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9cbbdc-9691-4c32-8b14-c327b012a963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((117, 11), (97, 11), (10, 11))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = \"/data/konrad/workspace/csv_files/\"\n",
    "data_type = \"loco_\"\n",
    "# data_type = \"splt_\"\n",
    "label_col = \"bilabel\"\n",
    "train_df = pd.read_csv(f\"{root_dir}{data_type}train_video_extracted_metadata.csv\", index_col=False)\n",
    "test_df = pd.read_csv(f\"{root_dir}{data_type}val_video_extracted_metadata.csv\", index_col=False)\n",
    "valid_df = pd.read_csv(f\"{root_dir}{data_type}test_video_extracted_metadata.csv\", index_col=False)\n",
    "labels = train_df[label_col].unique()\n",
    "label2id = {l:i for i, l in enumerate(labels)}\n",
    "id2label = {i:l for i, l in enumerate(labels)}\n",
    "# num_labels = len(labels)\n",
    "num_labels = 1\n",
    "train_df['target'] = train_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "test_df['target'] = test_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "valid_df['target'] = valid_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "test_df = test_df.sample(frac=.20).reset_index(drop=True)\n",
    "train_df.shape, test_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c4dff2a-1234-41da-b7d1-c5181dda950c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/konrad/x_env/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/data/konrad/x_env/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [400/400 2:30:31, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.069232</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.058362</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.069589</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.536842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.067530</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.673684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.068043</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.694737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.063334</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.773684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.063662</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.063568</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.773684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.082817</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.082386</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 12\n",
    "clip_len = 16\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "frame_rate = 15\n",
    "patience = 2\n",
    "\n",
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "train_dataset = CalfVideoDataset(train_df, image_processor, max_frames=clip_len, frame_rate = frame_rate)\n",
    "test_dataset = CalfVideoDataset(test_df, image_processor, max_frames=clip_len, frame_rate = frame_rate)\n",
    "valid_dataset = CalfVideoDataset(valid_df, image_processor, max_frames=clip_len, frame_rate = frame_rate)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience, verbose=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'../training_log/results/{model_name}',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'../training_log/logs/{model_name}',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_results = trainer.train()\n",
    "model.save_pretrained(f'../models/{model_name}_{clip_len}_{frame_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3d2d7f1-fd15-4c00-9e72-e534e4bc0ccc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/konrad/x_env/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/data/konrad/x_env/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 49:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.013869</td>\n",
       "      <td>0.268041</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.561728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.015725</td>\n",
       "      <td>0.164948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.015346</td>\n",
       "      <td>0.175258</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.506173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.014084</td>\n",
       "      <td>0.268041</td>\n",
       "      <td>0.219780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.561728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.536082</td>\n",
       "      <td>0.671533</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.567901</td>\n",
       "      <td>0.471451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.013816</td>\n",
       "      <td>0.556701</td>\n",
       "      <td>0.686131</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>0.580247</td>\n",
       "      <td>0.508873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.013687</td>\n",
       "      <td>0.567010</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.592593</td>\n",
       "      <td>0.515046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.013529</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.527392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.013497</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.527392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.013536</td>\n",
       "      <td>0.587629</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.527392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/konrad/x_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "clip_len = 16\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "frame_rate = 15\n",
    "patience = 2\n",
    "\n",
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "train_dataset = CalfVideoDataset(train_df, image_processor, max_frames=clip_len, frame_rate = frame_rate)\n",
    "test_dataset = CalfVideoDataset(test_df, image_processor, max_frames=clip_len, frame_rate = frame_rate)\n",
    "valid_dataset = CalfVideoDataset(valid_df, image_processor, max_frames=clip_len, frame_rate = frame_rate)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience, verbose=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'../training_log/results/{model_name}',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'../training_log/logs/{model_name}',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\",\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_results = trainer.train()\n",
    "model.save_pretrained(f'../models/{model_name}_{clip_len}_{frame_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0525ee-87e2-4bf5-94f1-be45b17c08d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.0010892752330983059, metrics={'train_runtime': 9109.9359, 'train_samples_per_second': 0.527, 'train_steps_per_second': 0.044, 'total_flos': 5.981056689084826e+18, 'train_loss': 0.0010892752330983059, 'epoch': 10.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "069bebc0-3b12-445e-aa47-d06eedd19b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=0.008067597635090351, metrics={'train_runtime': 3002.7585, 'train_samples_per_second': 0.39, 'train_steps_per_second': 0.033, 'total_flos': 1.4578825679644262e+18, 'train_loss': 0.008067597635090351, 'epoch': 10.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1248b6-cff4-4a72-96ed-7c9fc86b69d3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf59c7d8-38b9-40ce-9d21-c6f25fa91492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Metrics:\n",
      "Accuracy: 0.4000\n",
      "F1: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "balanced_accuracy: 0.3333\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics=compute_class_metrics\n",
    ")\n",
    "eval_results = trainer.evaluate(valid_dataset)\n",
    "\n",
    "metrics = eval_results\n",
    "print(\"Global Metrics:\")\n",
    "print(f\"Accuracy: {metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"F1: {metrics['eval_f1']:.4f}\")\n",
    "print(f\"Precision: {metrics['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['eval_recall']:.4f}\")\n",
    "print(f\"balanced_accuracy: {metrics['eval_balanced_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90bd4ede-0881-4757-a30c-5c2d24dabe0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Metrics:\n",
      "Accuracy: 0.4000\n",
      "F1: 0.4000\n",
      "Precision: 0.5000\n",
      "Recall: 0.3333\n",
      "balanced_accuracy: 0.4167\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics=compute_class_metrics\n",
    ")\n",
    "eval_results = trainer.evaluate(valid_dataset)\n",
    "\n",
    "metrics = eval_results\n",
    "print(\"Global Metrics:\")\n",
    "print(f\"Accuracy: {metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"F1: {metrics['eval_f1']:.4f}\")\n",
    "print(f\"Precision: {metrics['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['eval_recall']:.4f}\")\n",
    "print(f\"balanced_accuracy: {metrics['eval_balanced_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81006103-7c56-4cc2-b033-b882e03a5032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Metrics:\n",
      "Accuracy: 0.5000\n",
      "F1: 0.5000\n",
      "Precision: 0.5000\n",
      "Recall: 0.5000\n",
      "\n",
      "Metrics per Class:\n",
      "Pneumonie:\n",
      "F1: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Diarrhé:\n",
      "F1: 0.5000\n",
      "Precision: 0.4000\n",
      "Recall: 0.6667\n",
      "\n",
      "Healthy:\n",
      "F1: 0.7273\n",
      "Precision: 0.8000\n",
      "Recall: 0.6667\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMetrics per Class:\")\n",
    "print(\"Pneumonie:\")\n",
    "print(f\"F1: {metrics['eval_Pneumonie_f1-score']:.4f}\")\n",
    "print(f\"Precision: {metrics['eval_Pneumonie_precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['eval_Pneumonie_recall']:.4f}\")\n",
    "print(\"\\nDiarrhé:\")\n",
    "print(f\"F1: {metrics['eval_Diarrhé_f1-score']:.4f}\")\n",
    "print(f\"Precision: {metrics['eval_Diarrhé_precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['eval_Diarrhé_recall']:.4f}\")\n",
    "print(\"\\nHealthy:\")\n",
    "print(f\"F1: {metrics['eval_Healthy_f1-score']:.4f}\")\n",
    "print(f\"Precision: {metrics['eval_Healthy_precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['eval_Healthy_recall']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb1519f9-3055-4767-8dee-211c9f7c1280",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Dependances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666f0e23-1f58-4da0-b754-d814ae4b3f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 16:12:19.590717: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-15 16:12:20.678788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import av\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoImageProcessor, VideoMAEForVideoClassification, TrainingArguments, Trainer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local dep\n",
    "project_dir = '/data/konrad/'\n",
    "sys.path.insert(0, project_dir)\n",
    "from helpers.helpers import get_indices, load_local_model\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6387ec-50e8-4503-b1d6-6a41a71fe9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Video Dataset Class\n",
    "class CalfVideoDataset(Dataset):\n",
    "    def __init__(self, video_df, processor, max_frames=20, frame_rate = 4, transform = None):\n",
    "        self.video_df = video_df\n",
    "        self.processor = processor\n",
    "        self.max_frames = max_frames\n",
    "        self.frame_rate = frame_rate\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_df)\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.video_df[\"target\"]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row_video = self.video_df.loc[idx]\n",
    "        label = torch.tensor(row_video[\"target\"], dtype=torch.float32)\n",
    "        container = av.open(row_video[\"path\"])\n",
    "\n",
    "        video_stream = container.streams.video[0]\n",
    "\n",
    "        # Get the duration in time base units\n",
    "        duration_in_units = video_stream.duration\n",
    "        \n",
    "        # Get the time base\n",
    "        time_base = video_stream.time_base\n",
    "        \n",
    "        # Calculate the duration in seconds\n",
    "        duration_in_seconds = duration_in_units * time_base\n",
    "\n",
    "        # Sample 16 frames\n",
    "        fps = int(container.streams.video[0].average_rate)\n",
    "        seg_len = int(duration_in_seconds) * fps\n",
    "        sample_rate = min(int(seg_len / self.max_frames) - 1, self.frame_rate)\n",
    "\n",
    "        # sample max_frames frames\n",
    "        indices = sample_frame_indices(clip_len=self.max_frames, frame_sample_rate=sample_rate, seg_len=seg_len)\n",
    "        video = read_video_pyav(container=container, indices=indices)\n",
    "\n",
    "        # if self.transform:\n",
    "        #     video = self.transform(video.transpose(0, 3, 1, 2))\n",
    "        \n",
    "        inputs = self.processor(list(video), return_tensors=\"pt\")\n",
    "\n",
    "        return {\"pixel_values\": inputs[\"pixel_values\"][0], \"label\": label}\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        if self.train_dataset is None:\n",
    "            raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "        train_dataset = self.train_dataset \n",
    "        data_collator = self.data_collator\n",
    "\n",
    "        dataloader_params = {\n",
    "            \"batch_size\": self._train_batch_size,\n",
    "            \"collate_fn\": data_collator,\n",
    "            \"num_workers\": self.args.dataloader_num_workers,\n",
    "            \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "            \"persistent_workers\": self.args.dataloader_persistent_workers,\n",
    "            \"sampler\": ImbalancedDatasetSampler(train_dataset)\n",
    "        }\n",
    "\n",
    "        return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2424d25b-e0d2-4003-b5a3-86d08ddfc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a collate function to handle the feature extractor outputs\n",
    "def collate_fn(batch):\n",
    "    # print(torch.stack([item['pixel_values'] for item in batch]).shape)\n",
    "    return {\n",
    "        'pixel_values': torch.stack([item['pixel_values'] for item in batch]),\n",
    "        'labels': torch.tensor([item['label'] for item in batch])\n",
    "    }\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    # preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    if num_labels == 1:\n",
    "        # For binary classification, assume preds are probabilities and use threshold 0.5\n",
    "        preds = (pred.predictions > 0.5).astype(int)\n",
    "    else:\n",
    "        # For multi-class classification, take the argmax to get the predicted class labels\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    metric_type = \"binary\" if num_labels == 1 else \"macro\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=metric_type)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def compute_class_metrics(pred):\n",
    "    out = {}\n",
    "    labels = pred.label_ids\n",
    "    \n",
    "    if num_labels == 1:\n",
    "        # For binary classification, assume preds are probabilities and use threshold 0.5\n",
    "        preds = (pred.predictions > 0.5).astype(int)\n",
    "    else:\n",
    "        # For multi-class classification, take the argmax to get the predicted class labels\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    # preds = pred.predictions.argmax(-1)\n",
    "    metric_type = \"binary\" if num_labels == 1 else \"micro\"\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=metric_type)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    report = classification_report(labels, preds, target_names=id2label.values(), output_dict=True)\n",
    "\n",
    "    if num_labels > 1:\n",
    "        for class_name, metrics in report.items():\n",
    "            if class_name not in ('accuracy', 'macro avg', 'weighted avg'):\n",
    "                out[f\"{class_name}_precision\"] = metrics['precision']\n",
    "                out[f\"{class_name}_recall\"] = metrics['recall']\n",
    "                out[f\"{class_name}_f1-score\"] = metrics['f1-score']\n",
    "\n",
    "    out.update({\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    })\n",
    "    return out\n",
    "    \n",
    "def read_video_pyav(container, indices):\n",
    "    '''\n",
    "    Decode the video with PyAV decoder.\n",
    "    Args:\n",
    "        container (`av.container.input.InputContainer`): PyAV container.\n",
    "        indices (`List[int]`): List of frame indices to decode.\n",
    "    Returns:\n",
    "        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n",
    "    '''\n",
    "    frames = []\n",
    "    container.seek(0)\n",
    "    start_index = indices[0]\n",
    "    end_index = indices[-1]\n",
    "    for i, frame in enumerate(container.decode(video=0)):\n",
    "        if i > end_index:\n",
    "            break\n",
    "        if i >= start_index and i in indices:\n",
    "            frames.append(frame)\n",
    "    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n",
    "\n",
    "\n",
    "def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n",
    "    '''\n",
    "    Sample a given number of frame indices from the video.\n",
    "    Args:\n",
    "        clip_len (`int`): Total number of frames to sample.\n",
    "        frame_sample_rate (`int`): Sample every n-th frame.\n",
    "        seg_len (`int`): Maximum allowed index of sample's last frame.\n",
    "    Returns:\n",
    "        indices (`List[int]`): List of sampled frame indices\n",
    "    '''\n",
    "    converted_len = int(clip_len * frame_sample_rate)\n",
    "    end_idx = np.random.randint(converted_len, seg_len)\n",
    "    # start_idx = end_idx - converted_len\n",
    "    start_idx = 0\n",
    "    indices = np.linspace(start_idx, end_idx, num=clip_len)\n",
    "    indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04116f36-81c2-4403-800c-90a82c87c5fd",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f9cbbdc-9691-4c32-8b14-c327b012a963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((117, 11), (483, 11), (10, 11))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = \"/data/konrad/datasets/whole_mixed/\"\n",
    "data_type = \"loco_\"\n",
    "label_col = \"bilabel\"\n",
    "train_df = pd.read_csv(f\"{root_dir}{data_type}train_video_extracted_metadata.csv\", index_col=False)\n",
    "test_df = pd.read_csv(f\"{root_dir}{data_type}val_video_extracted_metadata.csv\", index_col=False)\n",
    "valid_df = pd.read_csv(f\"{root_dir}{data_type}test_video_extracted_metadata.csv\", index_col=False)\n",
    "labels = train_df[label_col].unique()\n",
    "label2id = {l:i for i, l in enumerate(labels)}\n",
    "id2label = {i:l for i, l in enumerate(labels)}\n",
    "# num_labels = len(labels)\n",
    "num_labels = 1\n",
    "train_df['target'] = train_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "test_df['target'] = test_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "valid_df['target'] = valid_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "train_df.shape, test_df.shape, valid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4dff2a-1234-41da-b7d1-c5181dda950c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/data/konrad/jupiter_env/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/data/konrad/jupiter_env/lib/python3.10/site-packages/transformers/feature_extraction_utils.py:142: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  return torch.tensor(value)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/100 03:30 < 34:40, 0.04 it/s, Epoch 1/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='41' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/41 35:20 < 47:49, 0.01 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 12\n",
    "clip_len = 16\n",
    "epochs = 10\n",
    "lr = 1e-5\n",
    "frame_rate = 15\n",
    "patience = 2\n",
    "\n",
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "train_dataset = CalfVideoDataset(train_df, image_processor, max_frames=clip_len, frame_rate = frame_rate)\n",
    "test_dataset = CalfVideoDataset(test_df, image_processor, max_frames=clip_len, frame_rate = frame_rate)\n",
    "valid_dataset = CalfVideoDataset(valid_df, image_processor, max_frames=clip_len, frame_rate = frame_rate)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience, verbose=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f'./results/{model_name}',\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'./logs/{model_name}',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"f1\"\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_results = trainer.train()\n",
    "model.save_pretrained(f'./models/{model_name}_{clip_len}_{frame_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1248b6-cff4-4a72-96ed-7c9fc86b69d3",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fa1f25c-054c-43e5-a4fa-78936eab89e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Metrics:\n",
      "Accuracy: 0.5000\n",
      "F1: 0.5000\n",
      "Precision: 0.5000\n",
      "Recall: 0.5000\n",
      "\n",
      "Metrics per Class:\n",
      "Pneumonie:\n",
      "F1: 0.0000\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "\n",
      "Diarrh√©:\n",
      "F1: 0.5000\n",
      "Precision: 0.4000\n",
      "Recall: 0.6667\n",
      "\n",
      "Healthy:\n",
      "F1: 0.7273\n",
      "Precision: 0.8000\n",
      "Recall: 0.6667\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    optimizers=(optimizer, scheduler),\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics=compute_class_metrics\n",
    ")\n",
    "eval_results = trainer.evaluate(valid_dataset)\n",
    "\n",
    "metrics = eval_results\n",
    "print(\"Global Metrics:\")\n",
    "print(f\"Accuracy: {metrics['eval_accuracy']:.4f}\")\n",
    "print(f\"F1: {metrics['eval_f1']:.4f}\")\n",
    "print(f\"Precision: {metrics['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['eval_recall']:.4f}\")\n",
    "\n",
    "print(\"\\nMetrics per Class:\")\n",
    "print(\"Pneumonie:\")\n",
    "print(f\"F1: {metrics['eval_Pneumonie_f1-score']:.4f}\")\n",
    "print(f\"Precision: {metrics['eval_Pneumonie_precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['eval_Pneumonie_recall']:.4f}\")\n",
    "print(\"\\nDiarrh√©:\")\n",
    "print(f\"F1: {metrics['eval_Diarrh√©_f1-score']:.4f}\")\n",
    "print(f\"Precision: {metrics['eval_Diarrh√©_precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['eval_Diarrh√©_recall']:.4f}\")\n",
    "print(\"\\nHealthy:\")\n",
    "print(f\"F1: {metrics['eval_Healthy_f1-score']:.4f}\")\n",
    "print(f\"Precision: {metrics['eval_Healthy_precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['eval_Healthy_recall']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jpt_env)",
   "language": "python",
   "name": "jupiter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

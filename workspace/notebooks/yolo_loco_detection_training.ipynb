{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebba3a0d-7eef-4848-a00b-9e06d5c80aa3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.12). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "\n",
    "# Local dep\n",
    "project_dir = '/data/konrad/workspace'\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "from helpers.datasets import CalfCenterFaceDataset\n",
    "from helpers.helpers import get_indices, uniform_sample_with_values, load_face_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf191360-2536-4052-a1ed-b2a26099c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bbox_gallery(image_df, n_cols=5):\n",
    "    n_images = image_df.shape[0]\n",
    "    n_rows = n_images // n_cols + int(n_images % n_cols > 0)\n",
    "    \n",
    "    plt.figure(figsize=(20, n_rows * 4))\n",
    "    for i, row in image_df.iterrows():\n",
    "        img = Image.open(row[\"path\"])\n",
    "        # plt.subplot(n_rows, n_cols, i + 1)\n",
    "        ax = plt.subplot(n_rows, n_cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        # ax = plt.gca()\n",
    "        score = row[\"conf\"]\n",
    "        box_x = row[\"box_x\"]\n",
    "        box_y = row[\"box_y\"]\n",
    "        box_width = row[\"box_width\"]\n",
    "        box_height = row[\"box_height\"]\n",
    "        box_x = box_x - (box_width / 2)\n",
    "        box_y = box_y - (box_height / 2)\n",
    "        rect = plt.Rectangle((box_x, box_y), box_width, box_height, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        plt.text(box_x, box_y - 10, f' {score:.2f}', color='red', fontsize=12, backgroundcolor='white')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def apply_transformations(image, bbox, transforms, label_id):\n",
    "    transformed = transforms(image=image, bboxes=[bbox], category_ids=[label_id])\n",
    "    \n",
    "    transformed_image = transformed['image']\n",
    "    transformed_bbox = transformed['bboxes'][0]\n",
    "    \n",
    "    return transformed_image, transformed_bbox\n",
    "    \n",
    "def df_to_yolo(df, root_dir, transforms = None, num_gen=3, img_src_dir = None):\n",
    "    \n",
    "    img_path = root_dir + \"/\" + \"images\"\n",
    "    if not os.path.exists(img_path):\n",
    "        os.makedirs(img_path)\n",
    "\n",
    "    lbl_path = root_dir + \"/\" + \"labels\"\n",
    "    if not os.path.exists(lbl_path):\n",
    "        os.makedirs(lbl_path)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Extract the image name\n",
    "        image_name = row['image']\n",
    "\n",
    "        img_src_dir = row['path']\n",
    "        \n",
    "        class_idx = row['target']\n",
    "\n",
    "        xmin = row['x_min']\n",
    "        ymin = row['y_min']\n",
    "        xmax = row['x_max']\n",
    "        ymax = row['y_max']\n",
    "\n",
    "        bbox = [xmin, ymin, xmax, ymax]\n",
    "        \n",
    "        if transforms is None:\n",
    "\n",
    "            shutil.copy(os.path.join(img_src_dir), os.path.join(img_path, image_name))\n",
    "            filename = os.path.splitext(image_name)[0]\n",
    "\n",
    "            # Open the image file\n",
    "            with Image.open(os.path.join(img_src_dir)) as img:\n",
    "            # Get the width and height\n",
    "                imgWidth, imgHeight = img.size\n",
    "\n",
    "            center_x = (xmin + xmax) / 2.0\n",
    "            center_y = (ymin + ymax) / 2.0\n",
    "    \n",
    "            width = xmax - xmin\n",
    "            height = ymax - ymin\n",
    "    \n",
    "            norm_center_x = center_x / imgWidth\n",
    "            norm_center_y = center_y / imgHeight\n",
    "            \n",
    "            norm_width = width / imgWidth\n",
    "            norm_height = height / imgHeight\n",
    "            \n",
    "            box_annotation = ' '.join([str(class_idx), str(norm_center_x), str(norm_center_y), str(norm_width), str(norm_height)])+'\\n'\n",
    "    \n",
    "            label_filename = lbl_path + '/' + filename + \".txt\"\n",
    "            anno_f = open(label_filename, 'w')\n",
    "            anno_f.writelines(box_annotation)\n",
    "            anno_f.close()\n",
    "\n",
    "            continue\n",
    "\n",
    "        original_image_path = os.path.join(img_src_dir)\n",
    "        img = cv2.imread(original_image_path)\n",
    "        imgHeight, imgWidth, _ = img.shape\n",
    "        \n",
    "        for i in range(num_gen):  # Apply transformations 5 times\n",
    "            transformed_image, transformed_bbox = apply_transformations(img, bbox, transforms, class_idx)\n",
    "            \n",
    "            center_x = (transformed_bbox[0] + transformed_bbox[2]) / 2.0\n",
    "            center_y = (transformed_bbox[1] + transformed_bbox[3]) / 2.0\n",
    "\n",
    "            width = transformed_bbox[2] - transformed_bbox[0]\n",
    "            height = transformed_bbox[3] - transformed_bbox[1]\n",
    "            \n",
    "            norm_center_x = center_x / imgWidth\n",
    "            norm_center_y = center_y / imgHeight\n",
    "            \n",
    "            norm_width = width / imgWidth\n",
    "            norm_height = height / imgHeight\n",
    "            \n",
    "            box_annotation = ' '.join([str(class_idx), str(norm_center_x), str(norm_center_y), str(norm_width), str(norm_height)]) + '\\n'\n",
    "            \n",
    "            transformed_image_name = f\"{os.path.splitext(image_name)[0]}_transformed_{i}.jpg\"\n",
    "            cv2.imwrite(os.path.join(img_path, transformed_image_name), transformed_image)\n",
    "            \n",
    "            label_filename = os.path.join(lbl_path, f\"{os.path.splitext(transformed_image_name)[0]}.txt\")\n",
    "            with open(label_filename, 'w') as anno_f:\n",
    "                anno_f.writelines(box_annotation)\n",
    "\n",
    "    print(\"Done !\")\n",
    "\n",
    "\n",
    "def delete_dir_if_exists(dir_path):\n",
    "    if os.path.exists(dir_path):\n",
    "        try:\n",
    "            shutil.rmtree(dir_path)\n",
    "            print(f\"Directory '{dir_path}' deleted\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error deleting directory '{dir_path}': {e}\")\n",
    "\n",
    "def set_yml(file_name, data_path, id2label, train_dir = \"train\", test_dir = \"test\", val_dir = \"val\"):\n",
    "    \n",
    "    # Create the dictionary structure for YAML\n",
    "    yolo_detect_config = {\n",
    "        'path': data_path,\n",
    "        'train': train_dir,\n",
    "        'test': test_dir,\n",
    "        'val': val_dir,\n",
    "        'names': id2label\n",
    "    }\n",
    "    \n",
    "    # Define the output file path\n",
    "    yml_file_path = os.path.join(data_path, f\"{file_name}.yml\")\n",
    "    \n",
    "    # Write the YAML file\n",
    "    with open(yml_file_path, 'w') as yml_file:\n",
    "        yaml.dump(yolo_detect_config, yml_file, default_flow_style=False)\n",
    "        \n",
    "    print(\"Yml file set !\")\n",
    "    return yml_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428afab2-bbe4-49a1-9bf1-83983c420bb7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = \"/data/konrad/workspace\"\n",
    "\n",
    "dataset_type = \"loco\"\n",
    "label_col = \"label\"\n",
    "\n",
    "num_labels = 3 if label_col == \"label\" else 1\n",
    "train_df = pd.read_csv(ROOT_DIR + f'/csv_files/mixed_10s_b0s_y7_1/{dataset_type}_train_image_extracted_metadata.csv', index_col=False)\n",
    "valid_df = pd.read_csv(ROOT_DIR + f'/csv_files/mixed_10s_b0s_y7_1/{dataset_type}_test_image_extracted_metadata.csv', index_col=False)\n",
    "test_df = pd.read_csv(ROOT_DIR + f'/csv_files/mixed_10s_b0s_y7_1/{dataset_type}_val_image_extracted_metadata.csv', index_col=False)\n",
    "labels = train_df[label_col].unique()\n",
    "label2id = {l:i for i, l in enumerate(labels)}\n",
    "id2label = {i:l for i, l in enumerate(labels)}\n",
    "train_df['target'] = train_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "test_df['target'] = test_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "valid_df['target'] = valid_df.apply(lambda row: label2id[row[label_col]], axis=1)\n",
    "test_df = test_df.sample(frac=.10).reset_index(drop=True)\n",
    "\n",
    "IMAGE_DIR = ROOT_DIR + f\"/datasets/yolo_detection_{dataset_type}\"\n",
    "num_gen = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334042a-a5a6-40b6-9db5-65f61cca203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Rotate(limit=(10, 20), p=.7),\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=(3,9), p=.7),\n",
    "        A.MedianBlur(blur_limit=7, p=.7),\n",
    "    ], p=.6),\n",
    "    A.Sharpen(p=.7),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))\n",
    "\n",
    "delete_dir_if_exists(IMAGE_DIR)\n",
    "df_to_yolo(train_df, IMAGE_DIR + '/train', img_src_dir = IMAGE_DIR, transforms = transforms, num_gen=num_gen)\n",
    "# df_to_yolo(train_df, IMAGE_DIR + '/train', img_src_dir = IMAGE_DIR, transforms = None, num_gen=num_gen)\n",
    "df_to_yolo(valid_df, IMAGE_DIR + '/test', img_src_dir = IMAGE_DIR)\n",
    "df_to_yolo(test_df, IMAGE_DIR + '/val', img_src_dir = IMAGE_DIR)\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8m.pt\")  # load a pretrained model (recommended for training)\n",
    "epoch = 10\n",
    "\n",
    "model_name = \"yolo_detect\"\n",
    "yml_file_path = set_yml(f\"{model_name}_{dataset_type}\", IMAGE_DIR, id2label)\n",
    "results = model.train(data=yml_file_path, epochs=epoch, project=ROOT_DIR + \"/training_log\", name=model_name, cfg=\"config.yml\")\n",
    "# results = model.train(data=\"yolo_detect.yml\", epochs=epoch, project=ROOT_DIR + \"/training_log\", name=\"yolo_detect\")\n",
    "results = model.val(data=yml_file_path, split='test')  # Evaluate on test set\n",
    "\n",
    "model.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c44195e-5516-45cb-af31-0271d95d6bd2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/data/konrad/workspace/datasets/yolo_detection_loco' deleted\n",
      "Done !\n",
      "Done !\n",
      "Done !\n",
      "Yml file set !\n",
      "New https://pypi.org/project/ultralytics/8.2.83 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.58 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24115MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/data/konrad/workspace/datasets/yolo_detection_loco/yolo_detect_loco.yml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=/data/konrad/workspace/training_log, name=yolo_detect3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/data/konrad/workspace/training_log/yolo_detect3\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3777433  ultralytics.nn.modules.head.Detect           [3, [192, 384, 576]]          \n",
      "Model summary: 295 layers, 25,858,057 parameters, 25,858,041 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /data/konrad/workspace/training_log/yolo_detect3', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /data/konrad/workspace/datasets/yolo_detection_loco/train/labels... 213 images, 0 backgrounds, 0 corrupt\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /data/konrad/workspace/datasets/yolo_detection_loco/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/konrad/workspace/datasets/yolo_detection_loco/val/labels... 694 images, 0 backgrounds, 0 corrupt: 10\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /data/konrad/workspace/datasets/yolo_detection_loco/val/labels.cache\n",
      "Plotting labels to /data/konrad/workspace/training_log/yolo_detect3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/data/konrad/workspace/training_log/yolo_detect3\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10       7.5G      1.609      7.909       1.78          5        640: 100%|██████████| 14/14 [00:02<00:00,  5.1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:02<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.298      0.409      0.195      0.125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10      7.26G       1.31      3.161      1.307          5        640: 100%|██████████| 14/14 [00:01<00:00,  8.9\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:02<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.252      0.807      0.315      0.197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10      7.37G      1.199      2.647      1.234          5        640: 100%|██████████| 14/14 [00:01<00:00,  9.0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:03<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694        0.2       0.64      0.237      0.148\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      7.38G      1.206      2.126      1.277          5        640: 100%|██████████| 14/14 [00:02<00:00,  6.3\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:03<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.276      0.824        0.3      0.193\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      7.34G      1.172      1.951      1.309          5        640: 100%|██████████| 14/14 [00:02<00:00,  6.3\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:03<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.275      0.736      0.323      0.217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      7.37G      1.162      1.775      1.307          5        640: 100%|██████████| 14/14 [00:02<00:00,  6.1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:02<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.296      0.634      0.324       0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      7.26G      1.062      1.593      1.202          5        640: 100%|██████████| 14/14 [00:01<00:00,  9.1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.276      0.727      0.334      0.243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      7.35G      1.018        1.5      1.194          5        640: 100%|██████████| 14/14 [00:01<00:00,  9.2\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.285       0.86      0.345       0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      7.26G     0.9985      1.442      1.163          5        640: 100%|██████████| 14/14 [00:01<00:00,  9.2\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.312      0.882      0.345      0.251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      7.33G     0.9484      1.349      1.119          5        640: 100%|██████████| 14/14 [00:01<00:00,  9.3\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.314       0.96      0.352      0.261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.015 hours.\n",
      "Optimizer stripped from /data/konrad/workspace/training_log/yolo_detect3/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from /data/konrad/workspace/training_log/yolo_detect3/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating /data/konrad/workspace/training_log/yolo_detect3/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.58 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24115MiB)\n",
      "Model summary (fused): 218 layers, 25,841,497 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:02<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.314      0.957      0.352      0.261\n",
      "              Diarrhea        376        376       0.51      0.984      0.604      0.452\n",
      "               Healthy        166        166      0.213      0.928      0.217      0.161\n",
      "             Pneumonia        152        152      0.217      0.961      0.236       0.17\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1m/data/konrad/workspace/training_log/yolo_detect3\u001b[0m\n",
      "Ultralytics YOLOv8.2.58 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24115MiB)\n",
      "Model summary (fused): 218 layers, 25,841,497 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/konrad/workspace/datasets/yolo_detection_loco/test/labels... 283 images, 0 backgrounds, 0 corrupt: 1\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /data/konrad/workspace/datasets/yolo_detection_loco/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:02<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        283        283      0.303      0.914      0.357      0.245\n",
      "              Diarrhea         99         99       0.32      0.909      0.358      0.243\n",
      "               Healthy        109        109      0.354      0.954      0.466      0.322\n",
      "             Pneumonia         75         75      0.235       0.88      0.246      0.171\n",
      "Speed: 0.1ms preprocess, 4.3ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1m/data/konrad/workspace/training_log/yolo_detect32\u001b[0m\n",
      "Ultralytics YOLOv8.2.58 🚀 Python-3.10.12 torch-2.3.1+cu121 CPU (AMD Ryzen Threadripper PRO 5975WX 32-Cores)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/data/konrad/workspace/training_log/yolo_detect3/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 7, 8400) (49.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.3.1+cu121...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ✅ 1.2s, saved as '/data/konrad/workspace/training_log/yolo_detect3/weights/best.torchscript' (99.1 MB)\n",
      "\n",
      "Export complete (2.8s)\n",
      "Results saved to \u001b[1m/data/konrad/workspace/training_log/yolo_detect3/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/data/konrad/workspace/training_log/yolo_detect3/weights/best.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=/data/konrad/workspace/training_log/yolo_detect3/weights/best.torchscript imgsz=640 data=/data/konrad/workspace/datasets/yolo_detection_loco/yolo_detect_loco.yml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/konrad/workspace/training_log/yolo_detect3/weights/best.torchscript'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Rotate(limit=(10, 20), p=.7),\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=(3,9), p=.7),\n",
    "        A.MedianBlur(blur_limit=7, p=.7),\n",
    "    ], p=.6),\n",
    "    A.Sharpen(p=.7),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))\n",
    "\n",
    "\n",
    "delete_dir_if_exists(IMAGE_DIR)\n",
    "# df_to_yolo(train_df, IMAGE_DIR + '/train', img_src_dir = IMAGE_DIR, transforms = transforms, num_gen=num_gen)\n",
    "df_to_yolo(train_df, IMAGE_DIR + '/train', img_src_dir = IMAGE_DIR, transforms = None, num_gen=num_gen)\n",
    "df_to_yolo(valid_df, IMAGE_DIR + '/test', img_src_dir = IMAGE_DIR)\n",
    "df_to_yolo(test_df, IMAGE_DIR + '/val', img_src_dir = IMAGE_DIR)\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8m.pt\")  # load a pretrained model (recommended for training)\n",
    "epoch = 10\n",
    "\n",
    "model_name = \"yolo_detect\"\n",
    "yml_file_path = set_yml(f\"{model_name}_{dataset_type}\", IMAGE_DIR, id2label)\n",
    "# results = model.train(data=yml_file_path, epochs=epoch, project=ROOT_DIR + \"/training_log\", name=\"yolo_detect\", cfg=\"config.yml\")\n",
    "results = model.train(data=yml_file_path, epochs=epoch, project=ROOT_DIR + \"/training_log\", name=model_name)\n",
    "results = model.val(data=yml_file_path, split='test')  # Evaluate on test set\n",
    "\n",
    "model.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5be0c21-cc03-4cf3-99ea-f291147b3135",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/data/konrad/workspace/datasets/yolo_detection_loco' deleted\n",
      "Done !\n",
      "Done !\n",
      "Done !\n",
      "Yml file set !\n",
      "New https://pypi.org/project/ultralytics/8.2.83 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.2.58 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24115MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8m.pt, data=/data/konrad/workspace/datasets/yolo_detection_loco/yolo_detect_loco.yml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=/data/konrad/workspace/training_log, name=yolo_detect4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0, hsv_s=0, hsv_v=0, degrees=0.0, translate=0, scale=0, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.8, bgr=0.0, mosaic=0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0, crop_fraction=1, cfg=config.yml, tracker=botsort.yaml, save_dir=/data/konrad/workspace/training_log/yolo_detect4\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1392  ultralytics.nn.modules.conv.Conv             [3, 48, 3, 2]                 \n",
      "  1                  -1  1     41664  ultralytics.nn.modules.conv.Conv             [48, 96, 3, 2]                \n",
      "  2                  -1  2    111360  ultralytics.nn.modules.block.C2f             [96, 96, 2, True]             \n",
      "  3                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
      "  4                  -1  4    813312  ultralytics.nn.modules.block.C2f             [192, 192, 4, True]           \n",
      "  5                  -1  1    664320  ultralytics.nn.modules.conv.Conv             [192, 384, 3, 2]              \n",
      "  6                  -1  4   3248640  ultralytics.nn.modules.block.C2f             [384, 384, 4, True]           \n",
      "  7                  -1  1   1991808  ultralytics.nn.modules.conv.Conv             [384, 576, 3, 2]              \n",
      "  8                  -1  2   3985920  ultralytics.nn.modules.block.C2f             [576, 576, 2, True]           \n",
      "  9                  -1  1    831168  ultralytics.nn.modules.block.SPPF            [576, 576, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  2   1993728  ultralytics.nn.modules.block.C2f             [960, 384, 2]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  2    517632  ultralytics.nn.modules.block.C2f             [576, 192, 2]                 \n",
      " 16                  -1  1    332160  ultralytics.nn.modules.conv.Conv             [192, 192, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  2   1846272  ultralytics.nn.modules.block.C2f             [576, 384, 2]                 \n",
      " 19                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  2   4207104  ultralytics.nn.modules.block.C2f             [960, 576, 2]                 \n",
      " 22        [15, 18, 21]  1   3777433  ultralytics.nn.modules.head.Detect           [3, [192, 384, 576]]          \n",
      "Model summary: 295 layers, 25,858,057 parameters, 25,858,041 gradients, 79.1 GFLOPs\n",
      "\n",
      "Transferred 469/475 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /data/konrad/workspace/training_log/yolo_detect4', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /data/konrad/workspace/datasets/yolo_detection_loco/train/labels... 1065 images, 0 backgrounds, 0 corrup\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /data/konrad/workspace/datasets/yolo_detection_loco/train/labels.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/konrad/workspace/datasets/yolo_detection_loco/val/labels... 694 images, 0 backgrounds, 0 corrupt: 10\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /data/konrad/workspace/datasets/yolo_detection_loco/val/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /data/konrad/workspace/training_log/yolo_detect4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 77 weight(decay=0.0), 84 weight(decay=0.0005), 83 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/data/konrad/workspace/training_log/yolo_detect4\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10      8.04G      1.535      3.654      1.705          9        640: 100%|██████████| 67/67 [00:08<00:00,  8.0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.548      0.437      0.285      0.131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10       7.4G      1.198      1.521      1.393          9        640: 100%|██████████| 67/67 [00:07<00:00,  8.8\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.176      0.202     0.0703     0.0381\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10       7.4G      1.162      1.408      1.354          9        640: 100%|██████████| 67/67 [00:07<00:00,  9.0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.227      0.457      0.234      0.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10      7.37G      1.025      1.358      1.252          9        640: 100%|██████████| 67/67 [00:07<00:00,  9.0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.284      0.539      0.288      0.132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10      7.35G     0.8777        1.2      1.142          9        640: 100%|██████████| 67/67 [00:07<00:00,  9.1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.309      0.672       0.34       0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10      7.39G     0.7449      1.105      1.056          9        640: 100%|██████████| 67/67 [00:07<00:00,  9.0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.292      0.801      0.358      0.204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10      7.28G     0.6383     0.9425     0.9934          9        640: 100%|██████████| 67/67 [00:07<00:00,  9.1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.319      0.759      0.358      0.248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      7.38G     0.5601     0.7907     0.9425          9        640: 100%|██████████| 67/67 [00:07<00:00,  9.1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.335      0.707      0.381      0.276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10      7.37G     0.4745     0.5568     0.9008          9        640: 100%|██████████| 67/67 [00:07<00:00,  9.0\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.331      0.798      0.367      0.275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10      7.39G     0.3949     0.4083     0.8665          9        640: 100%|██████████| 67/67 [00:07<00:00,  9.1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:01<"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.329      0.763      0.357      0.259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.062 hours.\n",
      "Optimizer stripped from /data/konrad/workspace/training_log/yolo_detect4/weights/last.pt, 52.0MB\n",
      "Optimizer stripped from /data/konrad/workspace/training_log/yolo_detect4/weights/best.pt, 52.0MB\n",
      "\n",
      "Validating /data/konrad/workspace/training_log/yolo_detect4/weights/best.pt...\n",
      "Ultralytics YOLOv8.2.58 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24115MiB)\n",
      "Model summary (fused): 218 layers, 25,841,497 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 22/22 [00:02<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        694        694      0.334      0.715      0.381      0.276\n",
      "              Diarrhea        376        376      0.558       0.79      0.624      0.463\n",
      "               Healthy        166        166      0.223      0.783      0.272      0.197\n",
      "             Pneumonia        152        152      0.222      0.572      0.245      0.168\n",
      "Speed: 0.1ms preprocess, 0.8ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1m/data/konrad/workspace/training_log/yolo_detect4\u001b[0m\n",
      "Ultralytics YOLOv8.2.58 🚀 Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24115MiB)\n",
      "Model summary (fused): 218 layers, 25,841,497 parameters, 0 gradients, 78.7 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /data/konrad/workspace/datasets/yolo_detection_loco/test/labels... 283 images, 0 backgrounds, 0 corrupt: 1\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /data/konrad/workspace/datasets/yolo_detection_loco/test/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 18/18 [00:02<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        283        283      0.313      0.746      0.311      0.224\n",
      "              Diarrhea         99         99      0.348      0.576      0.344      0.248\n",
      "               Healthy        109        109      0.363      0.822      0.358      0.254\n",
      "             Pneumonia         75         75      0.229       0.84       0.23      0.171\n",
      "Speed: 0.1ms preprocess, 2.8ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1m/data/konrad/workspace/training_log/yolo_detect42\u001b[0m\n",
      "Ultralytics YOLOv8.2.58 🚀 Python-3.10.12 torch-2.3.1+cu121 CPU (AMD Ryzen Threadripper PRO 5975WX 32-Cores)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/data/konrad/workspace/training_log/yolo_detect4/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 7, 8400) (49.6 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m starting export with torch 2.3.1+cu121...\n",
      "\u001b[34m\u001b[1mTorchScript:\u001b[0m export success ✅ 1.4s, saved as '/data/konrad/workspace/training_log/yolo_detect4/weights/best.torchscript' (99.1 MB)\n",
      "\n",
      "Export complete (3.0s)\n",
      "Results saved to \u001b[1m/data/konrad/workspace/training_log/yolo_detect4/weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=/data/konrad/workspace/training_log/yolo_detect4/weights/best.torchscript imgsz=640  \n",
      "Validate:        yolo val task=detect model=/data/konrad/workspace/training_log/yolo_detect4/weights/best.torchscript imgsz=640 data=/data/konrad/workspace/datasets/yolo_detection_loco/yolo_detect_loco.yml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/data/konrad/workspace/training_log/yolo_detect4/weights/best.torchscript'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms = A.Compose([\n",
    "    A.Rotate(limit=(10, 20), p=.7),\n",
    "    A.OneOf([\n",
    "        A.GaussianBlur(blur_limit=(3,9), p=.7),\n",
    "        A.MedianBlur(blur_limit=7, p=.7),\n",
    "    ], p=.6),\n",
    "    A.Sharpen(p=.7),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['category_ids']))\n",
    "\n",
    "delete_dir_if_exists(IMAGE_DIR)\n",
    "df_to_yolo(train_df, IMAGE_DIR + '/train', img_src_dir = IMAGE_DIR, transforms = transforms, num_gen=num_gen)\n",
    "# df_to_yolo(train_df, IMAGE_DIR + '/train', img_src_dir = IMAGE_DIR, transforms = None, num_gen=num_gen)\n",
    "df_to_yolo(valid_df, IMAGE_DIR + '/test', img_src_dir = IMAGE_DIR)\n",
    "df_to_yolo(test_df, IMAGE_DIR + '/val', img_src_dir = IMAGE_DIR)\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8m.pt\")  # load a pretrained model (recommended for training)\n",
    "epoch = 10\n",
    "\n",
    "model_name = \"yolo_detect\"\n",
    "yml_file_path = set_yml(f\"{model_name}_{dataset_type}\", IMAGE_DIR, id2label)\n",
    "results = model.train(data=yml_file_path, epochs=epoch, project=ROOT_DIR + \"/training_log\", name=model_name, cfg=\"config.yml\")\n",
    "# results = model.train(data=\"yolo_detect.yml\", epochs=epoch, project=ROOT_DIR + \"/training_log\", name=\"yolo_detect\")\n",
    "results = model.val(data=yml_file_path, split='test')  # Evaluate on test set\n",
    "\n",
    "model.export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (main_env)",
   "language": "python",
   "name": "x_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

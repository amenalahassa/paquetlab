{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8201fbd-0c96-4385-8de9-215cc841c796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated RAM usage: 1.00 MB\n",
      "Estimated GPU usage: 17.15 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "def calculate_memory_usage(model, batch_size, input_size):\n",
    "    \"\"\"\n",
    "    Calculate the memory usage for a given model, batch size, and input size.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The neural network model (PyTorch model).\n",
    "    - batch_size: The size of the batches for training/inference.\n",
    "    - input_size: The size of the input tensor (e.g., (channels, height, width) for images).\n",
    "    \n",
    "    Returns:\n",
    "    - ram_usage: Estimated RAM usage in bytes.\n",
    "    - gpu_usage: Estimated GPU memory usage in bytes.\n",
    "    \"\"\"\n",
    "    # Ensure the model is on the CPU for initial RAM estimation\n",
    "    model = model.cpu()\n",
    "    \n",
    "    # Create a dummy input tensor with the specified batch size and input size\n",
    "    dummy_input = torch.randn(batch_size, *input_size)\n",
    "    \n",
    "    # Estimate the model size (parameters)\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    \n",
    "    # Estimate the forward pass activation size\n",
    "    def forward_hook(module, input, output):\n",
    "        activations.append(output)\n",
    "    \n",
    "    activations = []\n",
    "    hooks = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Module):\n",
    "            hooks.append(layer.register_forward_hook(forward_hook))\n",
    "    \n",
    "    # Perform a forward pass to populate the activations list\n",
    "    model(dummy_input)\n",
    "    \n",
    "    # Remove the hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    activation_size = sum(act.numel() * act.element_size() for act in activations)\n",
    "    \n",
    "    # Estimate the backward pass activation size (same as forward pass)\n",
    "    backward_activation_size = activation_size\n",
    "    \n",
    "    # Total RAM usage (model parameters + activations for forward and backward pass)\n",
    "    ram_usage = param_size + (activation_size + backward_activation_size)\n",
    "    \n",
    "    # GPU usage estimation (if using a GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        dummy_input = dummy_input.cuda()\n",
    "        \n",
    "        # Clear any existing memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Perform a forward and backward pass to estimate GPU memory usage\n",
    "        gpu_memory_before = torch.cuda.memory_allocated()\n",
    "        output = model(dummy_input)\n",
    "        loss = output.sum()\n",
    "        loss.backward()\n",
    "        gpu_memory_after = torch.cuda.memory_allocated()\n",
    "        \n",
    "        gpu_usage = gpu_memory_after - gpu_memory_before\n",
    "\n",
    "        # #Free GPU resources\n",
    "        del dummy_input\n",
    "        del output\n",
    "        del loss\n",
    "        model = model.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        gpu_usage = None\n",
    "    \n",
    "    return ram_usage, gpu_usage\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a simple model for demonstration\n",
    "    class SimpleModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleModel, self).__init__()\n",
    "            self.fc1 = nn.Linear(784, 256)\n",
    "            self.fc2 = nn.Linear(256, 128)\n",
    "            self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    model = SimpleModel()\n",
    "    batch_size = 32\n",
    "    input_size = (1, 28, 28)  # Example for MNIST dataset\n",
    "    \n",
    "    ram_usage, gpu_usage = calculate_memory_usage(model, batch_size, input_size)\n",
    "    \n",
    "    print(f\"Estimated RAM usage: {ram_usage / (1024 ** 2):.2f} MB\")\n",
    "    print(f\"Estimated GPU usage: {gpu_usage / (1024 ** 2):.2f} MB\" if gpu_usage else \"GPU not available\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1d81bf-c741-40d0-af79-dda9a7acb1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f84eda7-5fc2-4e16-a751-fef18bc6b4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c23208-bf15-406c-8829-07ae35ff3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.90 MB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# Calculate the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "model_size_MB = num_params * 4 / (1024 ** 2)  # Model size in MB\n",
    "print(f\"Model size: {model_size_MB:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4afdb6-5d66-4738-8c1a-d3b38524d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def calculate_memory_usage(model, batch_size, input_size):\n",
    "    \"\"\"\n",
    "    Calculate the memory usage for a given model, batch size, and input size.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The neural network model (PyTorch model).\n",
    "    - batch_size: The size of the batches for training/inference.\n",
    "    - input_size: The size of the input tensor (e.g., (channels, height, width) for images).\n",
    "    \n",
    "    Returns:\n",
    "    - ram_usage_training: Estimated RAM usage in bytes during training.\n",
    "    - ram_usage_inference: Estimated RAM usage in bytes during inference.\n",
    "    - gpu_usage_training: Estimated GPU memory usage in bytes during training.\n",
    "    - gpu_usage_inference: Estimated GPU memory usage in bytes during inference.\n",
    "    \"\"\"\n",
    "    # Ensure the model is on the CPU for initial RAM estimation\n",
    "    model = model.cpu()\n",
    "    \n",
    "    # Create a dummy input tensor with the specified batch size and input size\n",
    "    dummy_input = torch.randn(batch_size, *input_size)\n",
    "    \n",
    "    # Estimate the model size (parameters)\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    \n",
    "    # Estimate the forward pass activation size\n",
    "    def forward_hook(module, input, output):\n",
    "        activations.append(output)\n",
    "    \n",
    "    activations = []\n",
    "    hooks = []\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, nn.Module):\n",
    "            hooks.append(layer.register_forward_hook(forward_hook))\n",
    "    \n",
    "    # Perform a forward pass to populate the activations list\n",
    "    model(dummy_input)\n",
    "    \n",
    "    # Remove the hooks\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "    \n",
    "    activation_size = sum(act.numel() * act.element_size() for act in activations)\n",
    "    \n",
    "    # Estimate the backward pass activation size (same as forward pass)\n",
    "    backward_activation_size = activation_size\n",
    "    \n",
    "    # Total RAM usage during training (model parameters + activations for forward and backward pass)\n",
    "    ram_usage_training = param_size + (activation_size + backward_activation_size)\n",
    "    \n",
    "    # Total RAM usage during inference (model parameters + activations for forward pass only)\n",
    "    ram_usage_inference = param_size + activation_size\n",
    "    \n",
    "    # GPU usage estimation (if using a GPU)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "        dummy_input = dummy_input.cuda()\n",
    "        \n",
    "        # Clear any existing memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Perform a forward pass to estimate GPU memory usage during inference\n",
    "        gpu_memory_before = torch.cuda.memory_allocated()\n",
    "        output = model(dummy_input)\n",
    "        gpu_memory_after = torch.cuda.memory_allocated()\n",
    "        \n",
    "        gpu_usage_inference = gpu_memory_after - gpu_memory_before\n",
    "        \n",
    "        # Perform a backward pass to estimate GPU memory usage during training\n",
    "        loss = output.sum()\n",
    "        gpu_memory_before = torch.cuda.memory_allocated()\n",
    "        loss.backward()\n",
    "        gpu_memory_after = torch.cuda.memory_allocated()\n",
    "        \n",
    "        gpu_usage_training = gpu_memory_after - gpu_memory_before\n",
    "        \n",
    "        # Free GPU resources\n",
    "        del dummy_input\n",
    "        del output\n",
    "        del loss\n",
    "        model = model.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        gpu_usage_training = None\n",
    "        gpu_usage_inference = None\n",
    "    \n",
    "    return ram_usage_training, ram_usage_inference, gpu_usage_training, gpu_usage_inference\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define a simple model for demonstration\n",
    "    class SimpleModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(SimpleModel, self).__init__()\n",
    "            self.fc1 = nn.Linear(784, 256)\n",
    "            self.fc2 = nn.Linear(256, 128)\n",
    "            self.fc3 = nn.Linear(128, 10)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = torch.relu(self.fc2(x))\n",
    "            x = self.fc3(x)\n",
    "            return x\n",
    "    \n",
    "    model = SimpleModel()\n",
    "    batch_size = 32\n",
    "    input_size = (1, 28, 28)  # Example for MNIST dataset\n",
    "    \n",
    "    # ram_usage_training, ram_usage_inference, gpu_usage_training, gpu_usage_inference = calculate_memory_usage(model, batch_size, input_size)\n",
    "    \n",
    "    # print(f\"Estimated RAM usage during training: {ram_usage_training / (1024 ** 2):.2f} MB\")\n",
    "    # print(f\"Estimated RAM usage during inference: {ram_usage_inference / (1024 ** 2):.2f} MB\")\n",
    "    # print(f\"Estimated GPU usage during training: {gpu_usage_training / (1024 ** 2):.2f} MB\" if gpu_usage_training else \"GPU not available\")\n",
    "    # print(f\"Estimated GPU usage during inference: {gpu_usage_inference / (1024 ** 2):.2f} MB\" if gpu_usage_inference else \"GPU not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "758f1dd5-a44b-42e0-8af2-049ef489a530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 12:21:47.215868: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-07 12:21:47.734975: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/konrad/jupiter_env/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2024-06-07 12:21:48.339170: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-07 12:21:48.356520: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-06-07 12:21:48.387296: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:104] Profiler session initializing.\n",
      "2024-06-07 12:21:48.387314: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:119] Profiler session started.\n",
      "2024-06-07 12:21:48.387348: I external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1239] Profiler found 1 GPUs\n",
      "2024-06-07 12:21:48.410097: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:163] cuptiEnableCallback: error 1: CUPTI_ERROR_INVALID_PARAMETER\n",
      "2024-06-07 12:21:48.410109: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:213] CuptiErrorManager is disabling profiling automatically.\n",
      "2024-06-07 12:21:48.410113: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:213] CuptiErrorManager is disabling profiling automatically.\n",
      "2024-06-07 12:21:48.410120: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-06-07 12:21:48.410124: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1288] function cupti_interface_->EnableCallback( 1 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.0928 - loss: 2.6075   \n",
      "Epoch 2/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1472 - loss: 2.3491 \n",
      "Epoch 3/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1347 - loss: 2.2571 \n",
      "Epoch 4/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.1770 - loss: 2.2080 \n",
      "Epoch 5/5\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.2131 - loss: 2.1673 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-07 12:21:49.217694: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:70] Profiler session collecting data.\n",
      "2024-06-07 12:21:49.218570: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:153] cuptiEnableCallback: ignored due to a previous error.\n",
      "2024-06-07 12:21:49.218583: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-06-07 12:21:49.218587: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1310] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error \n",
      "2024-06-07 12:21:49.218592: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:142] cuptiFinalize: ignored due to a previous error.\n",
      "2024-06-07 12:21:49.218595: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:223] cuptiGetResultString: ignored due to a previous error.\n",
      "2024-06-07 12:21:49.218598: E external/local_xla/xla/backends/profiler/gpu/cupti_tracer.cc:1373] function cupti_interface_->Finalize()failed with error \n",
      "2024-06-07 12:21:49.240381: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\n",
      "2024-06-07 12:21:49.240402: E external/local_xla/xla/backends/profiler/gpu/cupti_error_manager.cc:135] cuptiGetTimestamp: ignored due to a previous error.\n",
      "2024-06-07 12:21:49.240408: I external/local_xla/xla/backends/profiler/gpu/cupti_collector.cc:540]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2024-06-07 12:21:49.254296: I external/local_tsl/tsl/profiler/lib/profiler_session.cc:131] Profiler session tear down.\n",
      "2024-06-07 12:21:49.255914: I external/local_tsl/tsl/profiler/rpc/client/save_profile.cc:144] Collecting XSpace to repository: logdir/plugins/profile/2024_06_07_12_21_49/alita.xplane.pb\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Example model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(256, input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Generate example data\n",
    "num_samples = 1000\n",
    "x_train = np.random.rand(num_samples, 784)\n",
    "y_train = np.random.randint(10, size=num_samples)\n",
    "\n",
    "# Profiling with TensorBoard\n",
    "tf.profiler.experimental.start('logdir')\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "tf.profiler.experimental.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db83cbb3-01f9-4a1e-a159-3e1d05ca7a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-07 12:22:43.210220: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-07 12:22:43.666517: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-06-07 12:22:44.286313: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-06-07 12:22:44.302389: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.16.2 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=logdir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0993b3-604d-471b-940f-18622f3edab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2024-06-07 12:25:21 3241325:3241325 init.cpp:166] function cbapi->getCuptiStatus() failed with error CUPTI_ERROR_MULTIPLE_SUBSCRIBERS_NOT_SUPPORTED (39)\n",
      "WARNING:2024-06-07 12:25:21 3241325:3241325 init.cpp:167] CUPTI initialization failed - CUDA profiler activities will be missing\n",
      "INFO:2024-06-07 12:25:21 3241325:3241325 init.cpp:169] If you see CUPTI_ERROR_INSUFFICIENT_PRIVILEGES, refer to https://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti\n",
      "STAGE:2024-06-07 12:25:21 3241325:3241325 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                  Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "       model_inference        20.15%     327.000us       100.00%       1.623ms       1.623ms             1  \n",
      "         aten::flatten         5.36%      87.000us         6.35%     103.000us     103.000us             1  \n",
      "            aten::view         0.99%      16.000us         0.99%      16.000us      16.000us             1  \n",
      "          aten::linear         5.11%      83.000us        70.24%       1.140ms     380.000us             3  \n",
      "               aten::t         1.66%      27.000us         3.27%      53.000us      17.667us             3  \n",
      "       aten::transpose         1.23%      20.000us         1.60%      26.000us       8.667us             3  \n",
      "      aten::as_strided         0.37%       6.000us         0.37%       6.000us       1.000us             6  \n",
      "           aten::addmm        59.15%     960.000us        61.86%       1.004ms     334.667us             3  \n",
      "          aten::expand         0.62%      10.000us         0.62%      10.000us       3.333us             3  \n",
      "           aten::copy_         1.97%      32.000us         1.97%      32.000us      10.667us             3  \n",
      "----------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.623ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-06-07 12:25:22 3241325:3241325 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-07 12:25:22 3241325:3241325 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "input_data = torch.randn(batch_size, *input_size)\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        output = model(input_data)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb23601c-39fa-4249-9342-14e01314ae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /tmp/ipykernel_3241325/2924412536.py\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import profile\n",
    "\n",
    "@profile\n",
    "def train_model():\n",
    "    output = model(input_data)\n",
    "\n",
    "train_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jpt_env)",
   "language": "python",
   "name": "jupiter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
